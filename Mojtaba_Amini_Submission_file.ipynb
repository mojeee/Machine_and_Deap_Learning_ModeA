{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mojtaba_Amini_Submission_file.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "I did not find the gold labels for the test set, so the only way to check your performance on the test set is to make a submission on the [Kaggle website](https://www.kaggle.com/c/nlp-getting-started/submit). \n",
        "I made a function to create a submission file from a prediction vector, *download_predictions_to_file()*, feel free to use it. Alternatively, you can just check your performance on a subset of the training set. "
      ],
      "metadata": {
        "id": "swn7Mc9vQHqo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "490C2OoPs4PX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f9ec650-bdea-4def-9ee1-0aa28934257d"
      },
      "source": [
        "# This is a sample Python script.\n",
        "\n",
        "# Press Shift+F10 to execute it or replace it with your code.\n",
        "# Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
        "\"\"\"\n",
        "Today is December 21 \n",
        "Machine Learning Project for Course project of Machine Learning and Deep Learning\n",
        "This is my first try to do a complete python project as a Data Science Student\n",
        "Today is january 13, I add word clound and use SVM\n",
        "\"\"\"\n",
        "#import inline as inline\n",
        "import matplotlib\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS , ImageColorGenerator\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from PIL import Image\n",
        "import tensorflow #the backend used by Keras (there are different beckend)\n",
        "from tensorflow.keras.models import Sequential #import the type of mpdel: sequential (e.g., MLP)\n",
        "from tensorflow.keras.layers import Input, Dense #simple linear layer\n",
        "from tensorflow.keras.utils import to_categorical # transformation for classification labels\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
        "\n",
        "def cleanend_data(text):\n",
        "    # Use a breakpoint in the code line below to debug your script.\n",
        "    text = text.lower()\n",
        "    re.sub(r'http\\S+', '', text)\n",
        "\n",
        "    text_tokens = word_tokenize(text)\n",
        "    all_stopwords = stopwords.words('english')\n",
        "    more_stop=[\"https\",\"tco\",\"via\",\"s\",\"rt\",\"st\",\"w\",\"im\",\"re\",\"m\",\"d\",\"v\",\"a\",\"b\",\"c\",\"e\",\"f\",\n",
        "               \"g\",\"h\",\"lol\",\"l\",\"n\",\"o\",\"p\",\"k\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"ll\",\"ve\",\"tco\",\"nt\"]\n",
        "    all_stopwords.extend(more_stop)\n",
        "    tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
        "    text = (\" \").join(tokens_without_sw)\n",
        "\n",
        "    text = re.sub('#><=', '', text)\n",
        "    text = re.sub('[0-9]+', '', text)\n",
        "    text = re.sub('[\\,]', '', text)\n",
        "    text = re.sub('[\\#]', '', text)\n",
        "    text = re.sub('[\\'\\\"]', '', text)\n",
        "    text = re.sub('[\\.\\$\\@\\!\\*\\&\\?\\_\\__]', '', text)\n",
        "\n",
        "\n",
        "    '''stop_words =[\"'tis\", \"tis\", \"a\", \"amp\", \"30th\", \"22\", \"ye\", \"I\", \"you\", \"20th\", \"me\", \"get\", \"st\",\n",
        "                    \"sráid\", \"ll\", \"le\", \"tú\", \"bhí\", \"faithfully\", \"tír\",\"https\",\"lol\",\"im\"]\n",
        "    for a in stop_words:\n",
        "        text=text.replace(a, \"\")'''\n",
        "    return text\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_url = 'https://www.math.unipd.it/~dasan/disaster/'\n",
        "raw_data = pd.read_csv(data_url + 'train.csv', sep=\",\") \n",
        "test_data = pd.read_csv(data_url + 'test.csv', sep=\",\") "
      ],
      "metadata": {
        "id": "AgZXJeD8DKg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPzMfoAKxhxQ"
      },
      "source": [
        "# before everthing we need to assign flag for each part to work easier\n",
        "#reading the file\n",
        "read_flag=False\n",
        "# executing some cleaning on the raw data\n",
        "cleaning_flag=True\n",
        "# count plot of target value to check that is unbalance or not\n",
        "countplot_flag=False\n",
        "# plot word cloud to see what word are most frequent in the tweets\n",
        "wordcloud_flag=False\n",
        "tain_val_split_flag=True\n",
        "simple_logestic_flag=False\n",
        "gridcv_logestic_flag=False\n",
        "KNN_flag = False\n",
        "simple_SVM_flag=True\n",
        "gridcv_SVM_flag=False# it doesn't need train_test split\n",
        "NeuralNetworkFlag=False# it doesn't need train_test split\n",
        "optimization_NN_flag=False # it doesn't need train_test split\n",
        "trainData_evaluation_flag=True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if read_flag:\n",
        "    path = \"./nlp-getting-started/train.csv\"\n",
        "    print(path)\n",
        "    raw_data = pd.read_csv(path)\n",
        "    #print(raw_data.head(10))\n",
        "\n",
        "if cleaning_flag:\n",
        "    raw_data['text_clean'] = raw_data.text.map(cleanend_data)\n",
        "    del raw_data['text']\n",
        "    del raw_data['id']\n",
        "    del raw_data['location']\n",
        "    del raw_data['keyword']\n",
        "\n",
        "if countplot_flag:\n",
        "    sns.set_theme(style=\"darkgrid\")\n",
        "    sns.countplot(x='target', data=raw_data)\n",
        "    plt.show()\n",
        "\n",
        "if wordcloud_flag:\n",
        "    all_tweet = \" \".join(review for review in raw_data.text_clean)\n",
        "    mask = np.array(Image.open(\"./nlp-getting-started/twitter6.jpg\"))\n",
        "    wordcloud = WordCloud(background_color=\"white\", max_words=1000, mask=mask).generate(all_tweet)\n",
        "    #image_colors = ImageColorGenerator(mask)\n",
        "    plt.figure(figsize=[30,30])\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "    plt.savefig(\"./nlp-getting-started/wordcloud.png\", format=\"png\")\n",
        "\n",
        "y_data = raw_data['target'].copy()\n",
        "del raw_data['target']\n",
        "\n",
        "if tain_val_split_flag:\n",
        "    X_train, X_val, y_train, y_val = train_test_split(raw_data, y_data, train_size=0.75, random_state=123)\n",
        "    cv = CountVectorizer(stop_words=\"english\",min_df=4)\n",
        "    X = cv.fit_transform(X_train.text_clean)\n",
        "    column_name=cv.get_feature_names_out()\n",
        "    X_tr_vec=pd.DataFrame(X.toarray(),columns=column_name)\n",
        "    X_val = cv.transform(X_val.text_clean)\n",
        "    column_name = cv.get_feature_names_out()\n",
        "    X_val_vec=pd.DataFrame(X_val.toarray(),columns=column_name)"
      ],
      "metadata": {
        "id": "anuna4tPEkDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if simple_logestic_flag:\n",
        "    clf = LogisticRegression()\n",
        "    clf.fit(X_tr_vec, y_train)\n",
        "    y_train_pred_lr = clf.predict(X_tr_vec)\n",
        "    y_val_pred_lr = clf.predict(X_val_vec)\n",
        "    tr_acc = accuracy_score(y_train, y_train_pred_lr)\n",
        "    val_acc = accuracy_score(y_val, y_val_pred_lr)\n",
        "    print(\"the accuracy score of the train is : {}, and the accuracy score of validation is : {}\".format(tr_acc,val_acc))"
      ],
      "metadata": {
        "id": "niPtY9IdFnii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if gridcv_logestic_flag:\n",
        "    C = [0.001, 0.01, 0.1, 1.,1.5,2, 10]\n",
        "    tr_acc=[]\n",
        "    val_acc=[]\n",
        "    for c in C:\n",
        "        model = LogisticRegression(C=c,max_iter = 200)\n",
        "        model.fit(X_tr_vec, y_train)\n",
        "        y_train_pred_lr = model.predict(X_tr_vec)\n",
        "        y_val_pred_lr = model.predict(X_val_vec)\n",
        "        tr_acc.append(accuracy_score(y_train, y_train_pred_lr))\n",
        "        val_acc.append(accuracy_score(y_val, y_val_pred_lr))\n",
        "        print(f\"LR. C= {c}.\\tTrain ACC: {accuracy_score(y_train, y_train_pred_lr)}\\tVal Acc: {accuracy_score(y_val, y_val_pred_lr)}\")\n",
        "    fig = plt.figure(figsize=(6, 4))\n",
        "    plt.plot(C,tr_acc, label=\"train\")\n",
        "    plt.plot(C,val_acc,label=\"validation\")\n",
        "    plt.xlabel(\"Inverse of Regularization parameter\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"GridCV of Logistic Regression\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "fZLK7kDxFwrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if KNN_flag:\n",
        "    accuracy_values_train = []\n",
        "    accuracy_values_test = []\n",
        "    k_values = range(1, 15)\n",
        "    for k in k_values:\n",
        "        model = KNeighborsClassifier(n_neighbors=k)\n",
        "        model.fit(X_tr_vec, y_train)\n",
        "        y_pred_train = model.predict(X_tr_vec)\n",
        "        y_pred_val = model.predict(X_val_vec)\n",
        "        accuracy_values_train.append(accuracy_score(y_pred_train, y_train))\n",
        "        accuracy_values_test.append(accuracy_score(y_pred_val, y_val))\n",
        "    fig = plt.figure(figsize=(6, 4))\n",
        "    plt.plot(k_values, accuracy_values_train, label=\"train\")\n",
        "    plt.plot(k_values, accuracy_values_test, label=\"test\")\n",
        "    plt.xlabel(\"K\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "OGf6XNFjGAi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if simple_SVM_flag:\n",
        "    c=1\n",
        "    gama=0.1\n",
        "    clf = svm.SVC(kernel='rbf',C=c)\n",
        "    clf.fit(X_tr_vec, y_train)\n",
        "    y_train_pred_lr = clf.predict(X_tr_vec)\n",
        "    y_val_pred_lr = clf.predict(X_val_vec)\n",
        "    tr_acc = accuracy_score(y_train, y_train_pred_lr)\n",
        "    val_acc = accuracy_score(y_val, y_val_pred_lr)\n",
        "    print(f\"SVM method with parameter C: {c}\\tTrain ACC: {tr_acc}\\tVal Acc: {val_acc}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fq-S38f5GPSo",
        "outputId": "f1ec6bcd-12b0-4ab3-c6a8-399643b89b9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM method with parameter C: 1\tTrain ACC: 0.9153967419863374\tVal Acc: 0.8140756302521008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if gridcv_SVM_flag:\n",
        "    lr = svm.SVC(kernel='rbf')\n",
        "    cv = CountVectorizer(stop_words=\"english\", min_df=3)\n",
        "    X = cv.fit_transform(raw_data.text_clean)\n",
        "    column_name = cv.get_feature_names_out()\n",
        "    X_all = pd.DataFrame(X.toarray(), columns=column_name)\n",
        "    parameters = {'kernel': ('linear', 'rbf')}\n",
        "    #parameters = {'kernel': ('linear', 'rbf'), 'C': [0.1,1, 10],'gamma': [0.1,1, 10]}\n",
        "    clf = GridSearchCV(lr, parameters,scoring = \"accuracy\",cv = 4)\n",
        "    clf.fit(X_all, y_data)\n",
        "    print(\"best Parameter is: {}\".format(clf.best_params_))\n",
        "    print(\"best score is: {}\".format(clf.best_score_))\n"
      ],
      "metadata": {
        "id": "9mxQ0zA-GjNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if NeuralNetworkFlag:\n",
        "\n",
        "    cv = CountVectorizer(stop_words=\"english\",min_df=4)\n",
        "    X = cv.fit_transform(raw_data.text_clean)\n",
        "    column_name=cv.get_feature_names_out()\n",
        "    X_tr_vec_NN=pd.DataFrame(X.toarray(),columns=column_name)\n",
        "    feature_vector_length = X_tr_vec_NN.shape[1]\n",
        "    print(X_tr_vec_NN.shape[1])\n",
        "    num_classes = 2 # fake or not fake\n",
        "    y_train_NN = to_categorical(y_data, num_classes)\n",
        "\n",
        "    clf = Sequential()  # we first define how the \"model\" looks like\n",
        "    clf.add(Dense(input_dim=feature_vector_length, units=10, activation='relu'))  # input layer\n",
        "    clf.add(Dense(units=10, activation='relu'))  # input layer\n",
        "    clf.add(Dense(num_classes, activation='softmax'))  # output layer\n",
        "    print(clf.summary())\n",
        "    #plot_model(model, show_shapes=True)\n",
        "    # Configure the model and start training\n",
        "    clf.compile(loss='categorical_crossentropy',  # loss metric\n",
        "                  optimizer='sgd',  # optimizer\n",
        "                  metrics=['accuracy'])  # displayed metric\n",
        "    history=clf.fit(X_tr_vec_NN, y_train_NN, epochs=20, batch_size=4, verbose=1, validation_split=0.25)\n",
        "    # summarize history for accuracy\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "    # plt.ylim(0.8, 1)\n",
        "    plt.show()\n",
        "    # summarize history for accuracy\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "    # plt.ylim(0.8, 1)\n",
        "    plt.show()\n",
        "    # see the testing performance\n",
        "    #test_results = model.evaluate(X_test, y_test_cat, verbose=1)\n",
        "    #print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]}%')"
      ],
      "metadata": {
        "id": "3C1IBMLmHfAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if optimization_NN_flag:\n",
        "    cv = CountVectorizer(stop_words=\"english\",min_df=4)\n",
        "    X = cv.fit_transform(raw_data.text_clean)\n",
        "    column_name=cv.get_feature_names_out()\n",
        "    X_tr_vec_NNOP=pd.DataFrame(X.toarray(),columns=column_name)\n",
        "    feature_vector_length = X_tr_vec_NNOP.shape[1]\n",
        "    print(X_tr_vec_NNOP.shape[1])\n",
        "    num_classes = 2 # fake or not fake\n",
        "    y_train_NNOP = to_categorical(y_data, num_classes)\n",
        "\n",
        "    def create_model():\n",
        "        # create model\n",
        "        model = Sequential()\n",
        "        model.add(Dense(input_dim=feature_vector_length, units=10, activation='relu'))  # input layer\n",
        "        model.add(Dense(units=10, activation='relu'))  # input layer\n",
        "        model.add(Dense(num_classes, activation='softmax'))  # output layer\n",
        "        model.compile(loss='categorical_crossentropy',  # loss metric\n",
        "                      optimizer='sgd',  # optimizer\n",
        "                      metrics=['accuracy'])  # displayed metric\n",
        "        return model\n",
        "\n",
        "    model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "    # define the grid search parameters\n",
        "    batch_size = [4, 8, 16, 20]\n",
        "    epochs = [5,10, 20]\n",
        "    param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
        "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "    grid_result = grid.fit(X_tr_vec_NNOP, y_train_NNOP)\n",
        "    # summarize results\n",
        "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "    means = grid_result.cv_results_['mean_test_score']\n",
        "    stds = grid_result.cv_results_['std_test_score']\n",
        "    params = grid_result.cv_results_['params']\n",
        "    for mean, stdev, param in zip(means, stds, params):\n",
        "        print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "metadata": {
        "id": "8tweoqL3IDX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if trainData_evaluation_flag:\n",
        "\n",
        "    test_data['text_clean'] = test_data.text.map(cleanend_data)\n",
        "    del test_data['text']\n",
        "    del test_data['location']\n",
        "    del test_data['keyword']\n",
        "    out_data=test_data.copy()\n",
        "    del out_data[\"text_clean\"]\n",
        "    del test_data['id']\n",
        "    test_data = cv.transform(test_data.text_clean)\n",
        "    column_name = cv.get_feature_names_out()\n",
        "    X_test_vec=pd.DataFrame(test_data.toarray(),columns=column_name)\n",
        "    if ~ NeuralNetworkFlag:\n",
        "        out_data[\"target\"]=clf.predict(X_test_vec)\n",
        "    print(out_data.head(10))\n",
        "    out_data.to_csv('predict.csv', index=False)\n",
        "    from google.colab import files\n",
        "    files.download('prediction_file.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "UTnGYEtAI4Dl",
        "outputId": "f97fa45a-b1c2-4b45-80c5-49b7873e4366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id  target\n",
            "0   0       0\n",
            "1   2       1\n",
            "2   3       0\n",
            "3   9       0\n",
            "4  11       1\n",
            "5  12       1\n",
            "6  21       0\n",
            "7  22       0\n",
            "8  27       0\n",
            "9  29       0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_cbe6090e-e2ee-425d-b987-68d33a6d2a5f\", \"prediction_file.csv\", 22746)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ncvoxHehGLCR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}